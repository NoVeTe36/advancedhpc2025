{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejutKRPC398_",
        "outputId": "67799711-324f-4792-8ce1-956b7f20adf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.12/dist-packages (from numba) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numba\n",
        "from numba import *\n",
        "import numba.cuda\n",
        "from numba.cuda import *"
      ],
      "metadata": {
        "id": "PrfFldks4OqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = numba.cuda.get_current_device()\n",
        "print(\"\\n\\nMultiprocessors:\", device.MULTIPROCESSOR_COUNT)\n",
        "print(\"Max threads per block:\", device.MAX_THREADS_PER_BLOCK)\n",
        "my_sms = getattr(device, 'MULTIPROCESSOR_COUNT')\n",
        "my_cc = device.compute_capability\n",
        "cc_cores_per_SM_dict = {\n",
        "    (2,0) : 32,\n",
        "    (2,1) : 48,\n",
        "    (3,0) : 192,\n",
        "    (3,5) : 192,\n",
        "    (3,7) : 192,\n",
        "    (5,0) : 128,\n",
        "    (5,2) : 128,\n",
        "    (6,0) : 64,\n",
        "    (6,1) : 128,\n",
        "    (7,0) : 64,\n",
        "    (7,5) : 64,\n",
        "    (8,0) : 64,\n",
        "    (8,6) : 128,\n",
        "    (8,9) : 128,\n",
        "    (9,0) : 128,\n",
        "    (10,0) : 128,\n",
        "    (12,0) : 128\n",
        "    }\n",
        "cores_per_sm = cc_cores_per_SM_dict.get(my_cc)\n",
        "total_cores = cores_per_sm*my_sms\n",
        "print(\"GPU compute capability: \" , my_cc)\n",
        "print(\"GPU total number of SMs: \" , my_sms)\n",
        "print(\"total cores: \" , total_cores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv68pFPT4XDN",
        "outputId": "9a8ddf06-66be-46fe-b4a0-8ac5d8f38f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Multiprocessors: 40\n",
            "Max threads per block: 1024\n",
            "GPU compute capability:  (7, 5)\n",
            "GPU total number of SMs:  40\n",
            "total cores:  2560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_current_device()\n",
        "my_sms = getattr(device, 'MULTIPROCESSOR_COUNT')\n",
        "my_cc = device.compute_capability\n",
        "cc_cores_per_SM_dict = {\n",
        "    (2,0) : 32,\n",
        "    (2,1) : 48,\n",
        "    (3,0) : 192,\n",
        "    (3,5) : 192,\n",
        "    (3,7) : 192,\n",
        "    (5,0) : 128,\n",
        "    (5,2) : 128,\n",
        "    (6,0) : 64,\n",
        "    (6,1) : 128,\n",
        "    (7,0) : 64,\n",
        "    (7,5) : 64,\n",
        "    (8,0) : 64,\n",
        "    (8,6) : 128,\n",
        "    (8,9) : 128,\n",
        "    (9,0) : 128,\n",
        "    (10,0) : 128,\n",
        "    (12,0) : 128\n",
        "    }\n",
        "cores_per_sm = cc_cores_per_SM_dict.get(my_cc)\n",
        "total_cores = cores_per_sm*my_sms\n",
        "print(\"GPU compute capability: \" , my_cc)\n",
        "print(\"GPU total number of SMs: \" , my_sms)\n",
        "print(\"total cores: \" , total_cores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooqNMSB445CD",
        "outputId": "2ff743c4-d59f-4161-ab89-e6ac7ab260ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU compute capability:  (7, 5)\n",
            "GPU total number of SMs:  40\n",
            "total cores:  2560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numba.cuda\n",
        "\n",
        "def check_cuda_memory():\n",
        "    context = numba.cuda.current_context()\n",
        "    free_mem, total_mem = context.get_memory_info()\n",
        "    used_mem = total_mem - free_mem\n",
        "\n",
        "\n",
        "    total_mem_gb = total_mem / (1024**3)\n",
        "    free_mem_gb = free_mem / (1024**3)\n",
        "    used_mem_gb = used_mem / (1024**3)\n",
        "\n",
        "    print(f\"CUDA Device: {context.device.name}\")\n",
        "    print(f\"Total CUDA Memory: {total_mem_gb:.2f} GB\")\n",
        "    print(f\"Free CUDA Memory: {free_mem_gb:.2f} GB\")\n",
        "    print(f\"Used CUDA Memory: {used_mem_gb:.2f} GB\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_cuda_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntf-xLWhHjdX",
        "outputId": "2c42a8c6-e1a5-4c74-df8e-279eb1b84e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Device: b'Tesla T4'\n",
            "Total CUDA Memory: 14.74 GB\n",
            "Free CUDA Memory: 14.64 GB\n",
            "Used CUDA Memory: 0.10 GB\n"
          ]
        }
      ]
    }
  ]
}